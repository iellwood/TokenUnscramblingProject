"""
Code for the paper

"Token unscrambling in fixed-weight biological models of transformer attention"
I.T. Ellwood 2026

This file trains the transformers with scrambled attention layer. For the paper, 15 models were trained
for 40 epochs. The paper also analyzed 4 vanilla transformer models. These can be generated by setting

    use_modified_layer = False

4 such models were trained for the paper.

Note that setting

    separate_scramble_matrix_for_K_and_V = False

Will set M^K = M^V. This possibility was not discussed in the paper.


"""

import torch
from LearnedTransformerAttentionProject.Model.BatchFactory import BatchFactory
from LearnedTransformerAttentionProject.Model.ModifiedTransformer import ModifiedTransformer as Model
from utils.smoothedvar import SmoothedVarArray
import numpy as np
from utils.CheckpointManager import CheckpointManager
from utils.TimeManager import TimeManager
import time
import gc

assert torch.cuda.is_available(), 'CUDA not available'

def train_model(checkpoint_folder_name, use_modified_layer, separate_scramble_matrix_for_K_and_V, print_header=''):
    device = 'cuda'
    batch_size = 32
    NLL_smoothed = SmoothedVarArray()

    size_of_epoch = int(np.round(206112 / batch_size))

    number_of_epochs = 40
    number_of_training_steps = size_of_epoch * number_of_epochs

    max_gradient_norm = 1.0
    src_ids = np.load("iwslt_data/german_ids.npy")
    tgt_ids = np.load("iwslt_data/english_ids.npy")

    batch_factory = BatchFactory(
        "iwslt_data/german_ids.npy",
        "iwslt_data/english_ids.npy",
        "Tokenizers/bytelevel_tokenizer_GERMAN.json",
        "Tokenizers/bytelevel_tokenizer_ENGLISH.json",
        device=device
    )

    model = Model(
        src_vocab_size=10000,
        trg_vocab_size=10000,
        src_sequence_length=batch_factory.get_src_max_sequence_length(),
        tgt_sequence_length=batch_factory.get_tgt_max_sequence_length() + 1,  # The extra one here is for autoregression
        FA_sequence_length=20,
        use_modified_layer=use_modified_layer,
        separate_scramble_matrix_for_K_and_V=separate_scramble_matrix_for_K_and_V,
        embed_dim=512,
        num_heads=8,
        number_of_layers=6,
        modified_layer=3,
        dim_feedforward=2048,
        device=device
    )

    model.init_weights()

    model.to('cuda')
    model.train(True)

    optim = torch.optim.Adam(params=model.parameters(), lr=3e-5)
    scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=0.01, end_factor=1, total_iters=2000)

    time_manager = TimeManager()

    checkpoint_manager = CheckpointManager('ModelCheckpoints/', checkpoint_folder_name=checkpoint_folder_name)

    for training_step in range(number_of_training_steps + 1):

        # TRAINING STEP
        src, tgt, tgt_shifted = batch_factory.get_batch(batch_size)

        NLL = model.compute_NLL(src, tgt, tgt_shifted)

        # Training Step
        optim.zero_grad()
        t_0 = time.time()
        NLL.backward(retain_graph=False)
        optim.step()
        scheduler.step()

        NLL_smoothed.add(NLL.detach().cpu().numpy() * 1.0)
        time_manager.record_step_time()

        del NLL
        torch.cuda.empty_cache()

        gc.collect()

        if training_step % 100 == 0:

            # Print the current model performance

            nll = float(NLL_smoothed.value())
            print(
                print_header,
                'Step', training_step, 'of', number_of_training_steps,
                'Time elapsed =', time_manager.time_elapsed_string(),
                'Time remaining =', time_manager.time_remaining_string(number_of_training_steps - training_step),
                'NLL =', float(np.round(nll, 1)),
                'P(correct) =', np.exp(-float(nll))
            )

            if training_step % 10000 == 0 and training_step > 0:
                tgt_autoregression_ids = model.autoregression(src[:, :1])
                tgt_tokens_autoregression = batch_factory.tokenize_tgt(tgt_autoregression_ids)

                src_ids = list(src[:, 0].detach().cpu().numpy())
                src_tokens = batch_factory.tokenize_src(src_ids)

                tgt_ids = list(tgt[:, 0].detach().cpu().numpy())
                tgt_tokens = batch_factory.tokenize_tgt(tgt_ids)

                print()
                print('Example text')
                print('German:         ', src_tokens)
                print('English Correct:', tgt_tokens)
                print('English Model:  ', tgt_tokens_autoregression)
                print()

        if training_step % (size_of_epoch * 10) == 0:
            checkpoint_manager.save_model_checkpoint(model, training_step)

number_of_replicates = 15  # Number of models to train
for replicate_number in range(15):
    print('')
    print('')
    print('Training separate scramble matrix network. Replicate =', replicate_number)
    print('')
    print('')

    train_model(
        'FAModel_SeparateScrambleMatrices_replicate_' + str(replicate_number),
        use_modified_layer=True,
        separate_scramble_matrix_for_K_and_V=True,
        print_header='M_K != M_V. Replicate = ' + str(replicate_number)
    )
